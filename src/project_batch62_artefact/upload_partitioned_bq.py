import os
import pandas as pd
from datetime import datetime
from google.cloud import bigquery
from google.api_core.exceptions import NotFound

def upload_all_months_partitioned(year: int, dataset: str, table: str, project: str):
    year = int(year)
    client = bigquery.Client(project=project)
    data_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "data"))
    base_table_id = f"{project}.{dataset}.{table}"

    # üìå Sch√©ma standard de la table
    schema = [
        bigquery.SchemaField("date", "DATE"),
        bigquery.SchemaField("departement", "STRING"),
        bigquery.SchemaField("temperature", "FLOAT"),
        bigquery.SchemaField("ensoleillement", "FLOAT"),
    ]

    # üîç V√©rifier si la table existe
    try:
        client.get_table(base_table_id)
        print(f"üìÅ Table trouv√©e : {base_table_id}")
    except NotFound:
        print(f"üÜï Table non trouv√©e. Cr√©ation de : {base_table_id}")
        table_ref = bigquery.Table(base_table_id, schema=schema)
        table_ref.time_partitioning = bigquery.TimePartitioning(
            type_=bigquery.TimePartitioningType.MONTH,
            field="date",
        )
        client.create_table(table_ref)
        print("‚úÖ Table cr√©√©e avec partition mensuelle sur le champ 'date'")

    for month in range(1, 13):
        file_path = os.path.join(data_dir, f"{year}-{month:02d}-open-meteo.csv")
        if not os.path.exists(file_path):
            print(f"‚õî Fichier non trouv√© : {file_path}")
            continue

        df = pd.read_csv(file_path)
        if "date" not in df.columns:
            print(f"‚õî Colonne 'date' absente dans {file_path}")
            continue

        # üîß Forcer la date au premier jour du mois
        first_day = datetime(year, month, 1).date()
        df["date"] = first_day

        # üî¢ Partition cible : table$YYYYMM
        partition_suffix = f"{year}{month:02d}"
        partitioned_table_id = f"{base_table_id}${partition_suffix}"

        job_config = bigquery.LoadJobConfig(
            write_disposition="WRITE_TRUNCATE",
            source_format=bigquery.SourceFormat.PARQUET,
            schema=schema,
        )

        temp_parquet = f"/tmp/{year}-{month:02d}-open-meteo.parquet"
        df.to_parquet(temp_parquet, index=False)

        with open(temp_parquet, "rb") as f:
            job = client.load_table_from_file(f, partitioned_table_id, job_config=job_config)
            job.result()
            print(f"‚úÖ Charg√© {len(df)} lignes dans la partition {partition_suffix}")

        os.remove(temp_parquet)
